import os
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
from langchain_community.document_loaders import (
    PyPDFLoader, 
    Docx2txtLoader, 
    TextLoader, 
    CSVLoader,
    UnstructuredHTMLLoader
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

PERSIST_DIRECTORY = "./chroma_db"

embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def get_loader_by_extension(file_path: str):
    """–í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext == ".pdf":
        return PyPDFLoader(file_path)
    elif ext == ".docx":
        return Docx2txtLoader(file_path)
    elif ext == ".csv":
        # csv_args –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —á–∏—Ç–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        return CSVLoader(file_path, encoding='utf-8', csv_args={'delimiter': ','})
    elif ext in [".html", ".htm"]:
        return UnstructuredHTMLLoader(file_path)
    elif ext in [".txt", ".md", ".py", ".json", ".ini", ".log"]:
        # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º TextLoader —Å utf-8
        return TextLoader(file_path, encoding='utf-8')
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω
        return TextLoader(file_path, encoding='utf-8')

def add_document_to_index(file_path: str, user_id: int):
    """–°—á–∏—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –ª—é–±–æ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –±–∞–∑—É."""
    try:
        loader = get_loader_by_extension(file_path)
        documents = loader.load()
    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ (–≤–æ–∑–º–æ–∂–Ω–æ, —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∏–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ UTF-8): {e}")

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)

    if not chunks:
        return 0

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    for chunk in chunks:
        chunk.metadata["user_id"] = str(user_id)
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∑–Ω–∞–ª–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
        chunk.metadata["source"] = os.path.basename(file_path)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB
    vectordb = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embedding_function
    )
    vectordb.add_documents(chunks)
    
    return len(chunks)

def get_relevant_context(query: str, user_id: int, k: int = 5) -> str:
    """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞."""
    vectordb = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embedding_function
    )
    
    results = vectordb.similarity_search(
        query, 
        k=k, 
        filter={"user_id": str(user_id)}
    )
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    context_parts = []
    for doc in results:
        source_name = doc.metadata.get("source", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª")
        context_parts.append(f"üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}\n{doc.page_content}")

    return "\n\n---\n\n".join(context_parts)

def clear_user_memory(user_id: int):
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
    pass